{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4faxUGzhpEYp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Programming your first Neuronal Network"
      ],
      "metadata": {
        "id": "b9UnHokWPC8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Implementing Forward Propagation"
      ],
      "metadata": {
        "id": "bu3ecl-aQ68D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to this hands-on exercise. Here, you will implement **forward propagation** for a simple neural network (nn) using [NumPy](https://numpy.org/devdocs/user/).\n",
        "\n",
        "Forward Propagation is the first step in training a neural network, where the inputs flow through the network.\n",
        "\n",
        "Feel free to consult web references and, of course, use any previously shared materials.\n",
        "\n",
        "You will complete the function `forward_propagation(X,params)` using a nn architecture as follows: Linear -> ReLU -> Linear -> Sigmoid.\n",
        "\n",
        "**What will you learn?:**\n",
        "\n",
        "1.   How to implement the forward propagation step in a simple nn.\n",
        "2.   How data flows in simple nns using NumPy.\n",
        "3.   How to use ReLU and Sigmoid activationfunctions.\n",
        "4.   How to implement output layer computations.\n",
        "\n",
        "\n",
        "**Your task:**\n",
        "\n",
        "Implement a function for forward propagation that:\n",
        "\n",
        "1. Applies linear transformations.\n",
        "2. Uses ReLU on the hidden layer.\n",
        "3. Uses sigmoid to the output.\n",
        "\n",
        "\n",
        "Ensure to read each code cell and follow the prompts. At the end, run the tests and check your results.\n"
      ],
      "metadata": {
        "id": "XW7oCfBRRiPm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FKrcEIuHO9rT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Simulated data (input features)\n",
        "\n",
        "X = np.array([[0.1, 0.5],\n",
        "              [0.3, 0.9],\n",
        "              [0.7, 0.2]])\n",
        "\n",
        "#Simulated parameters of the network: weights and biases\n",
        "params = {'W1': np.array([[0.2, -0.5, 0.1],\n",
        "                    [0.4, 0.3, -0.2]]),\n",
        "          'b1': np.array([[0.1], [-0.3]]),\n",
        "          'W2': np.array([[0.7, -0.1]]),\n",
        "          'b2': np.array([[0.5]])\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To think:** What is the shape of each parameter? Why do they have this shape?"
      ],
      "metadata": {
        "id": "b5tEU3jWZYpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You can print the parameters' shapes\n",
        "\n",
        "for key, value in params.items():\n",
        "    print(f\"{key}: {value.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwXiQ09o0Lu9",
        "outputId": "721f8433-8cd1-4f7f-ffa1-b31ec8ae3521"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1: (2, 3)\n",
            "b1: (2, 1)\n",
            "W2: (1, 2)\n",
            "b2: (1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Activation functions\n",
        "\n",
        "def relu(Z):\n",
        "  \"\"\"\n",
        "  Applies the ReLU activation function.\n",
        "\n",
        "  Arguments:\n",
        "  Input array.\n",
        "\n",
        "  Returns:\n",
        "  ReLU applied to Z.\n",
        "  \"\"\"\n",
        "  return np.maximum(0, Z)\n",
        "\n",
        "def sigmoid(Z):\n",
        "  \"\"\"\n",
        "  Applies the Sigmoid activation function.\n",
        "\n",
        "  Arguments:\n",
        "  Input array.\n",
        "\n",
        "  Returns:\n",
        "  Sigmoid of Z.\n",
        "  \"\"\"\n",
        "  return 1 / (1 + np.exp(-Z))"
      ],
      "metadata": {
        "id": "SKe2TZOOZXL0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To think:**\n",
        "\n",
        "1. What does an activation funcion in a nn?\n",
        "2. What would happen if we do not use any activation function at all?\n"
      ],
      "metadata": {
        "id": "fJRZjaMIaehs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Forward propagation\n",
        "# Your task is to bring this neural network to life! Fill in the steps of forward\n",
        "# propagation by completing the function below.\n",
        "\n",
        "def forward_propagation(X, params):\n",
        "  \"\"\"\n",
        "  Performs forward propagation for a simple neural network:\n",
        "  Input X → Z1 = W1·X + b1 → A1 = ReLU(Z1) → Z2 = W2·A1 + b2 → Output A2 = Sigmoid(Z2)\n",
        "\n",
        "  Arguments:\n",
        "  X: input data of size (n,m)\n",
        "  parameters: python dictionary containing W1, b1, W2, b2.\n",
        "\n",
        "  Returns:\n",
        "  A2; the output of the sigmoid function.\n",
        "  \"\"\"\n",
        "  #Your code goes here :\n",
        "\n",
        "  A2 = #complete this line with the final output of the network\n",
        "  return A2\n"
      ],
      "metadata": {
        "id": "c2qg-9d0aYGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions**\n",
        "\n",
        "Complete the `forward_propagation` above using the following steps:\n",
        "\n",
        "\n",
        "\n",
        "1.   Compute the first linear step:\n",
        "`Z1 = W1 ⋅ X + b1`\n",
        "2.   Apply ReLU: `A1 = ReLU(Z1)`\n",
        "3.   Compute the second linear step: `Z2 = W1 ⋅ A1 + b2`\n",
        "4.   Apply sigmoid: `A2 = σ(Z2)`\n",
        "\n",
        "Yo can use the `relu` and `sigmoid` already provided.\n",
        "\n",
        "\n",
        "**To think:**\n",
        "\n",
        "1. What should you expect from the forward propagation function? What type of data does it take? What does it mean?\n",
        "2. If your values don't match the expected output, how would you identify which step went wrong?\n",
        "3. What would happen if we didn’t use any activation function at all?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "smu9XiGKcoxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test your Forward Propagation"
      ],
      "metadata": {
        "id": "IuD-0s47f67O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have completed the forward propagation exercise, you can test your implementation running the following cell."
      ],
      "metadata": {
        "id": "LqDsDSszhV8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run this test after every modification to your function.\n",
        "\n",
        "def test_forward_propagation(student_funct):\n",
        "  print(\"Running tests\")\n",
        "\n",
        "  try:\n",
        "\n",
        "      output = student_funct(X,params)\n",
        "      expected_output = np.array([[0.62901652, 0.61939945]]) #result\n",
        "\n",
        "      if output.shape != expected_output.shape: #Shape unit test\n",
        "        print(f\"Output shape does not match the expected shape. Expected {expected_output.shape}, got {output.shape}\")\n",
        "        return\n",
        "\n",
        "      if np.allclose(output, expected_output, atol= 1e-5): #Values unit test\n",
        "        print(\"Output is correct. Great job.\")\n",
        "        return\n",
        "      else: print(\"Output values are incorrect. Check your matrix operations and activations. Look at the shapes and the values.\")\n",
        "\n",
        "      if np.all(output >= 0) and np.all(output <= 1): #Values unit test\n",
        "        print(\"Sigmoid output is in the right interval\")\n",
        "        return\n",
        "      else: print(\"Sigmoid output should be between o and 1.\")\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "#Run test\n",
        "test_forward_propagation(forward_propagation)\n"
      ],
      "metadata": {
        "id": "XCFC-koYhEwa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "d17a6bbe-5b1e-4c7d-c18b-7537a4c37c14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'forward_propagation' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-1436006439.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#Run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtest_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'forward_propagation' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution: Instuctor only\n",
        "\n"
      ],
      "metadata": {
        "id": "4faxUGzhpEYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, params):\n",
        "    W1 = params[\"W1\"]\n",
        "    b1 = params[\"b1\"]\n",
        "    W2 = params[\"W2\"]\n",
        "    b2 = params[\"b2\"]\n",
        "\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    return A2\n",
        "\n",
        "A2 = forward_propagation(X, params)\n"
      ],
      "metadata": {
        "id": "uqsPWVvrpLk6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}